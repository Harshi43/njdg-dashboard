# -*- coding: utf-8 -*-
"""Archita_NJDG_Dashboard_Reimagined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNyY6LcogZKsLbQKPYGkcrePDwe-4shc

# üèõÔ∏è NJDG Dashboard Reimagined
## Case Management System for Karnataka High Court

**Team:** Lawgorithm
**Problem Statement:** PS1 - Reimagining the National Judicial Data Grid (NJDG) Dashboard for Case Management

---

## üì¶ 1. Setup & Installation
"""


# Import libraries
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.calibration import CalibratedClassifierCV
import joblib


warnings.filterwarnings('ignore')

print("‚úÖ All libraries loaded successfully!")

"""---

## üìä 2. Data Loading
"""

# Load Cases Dataset
cases_url = 'https://drive.google.com/uc?id=1f38FA3WzDO2gs5sPsF34ckmAeDgbbrOJ'
df_cases = pd.read_csv(cases_url)

# Load Hearings Dataset
hearings_url = 'https://drive.google.com/uc?id=12PgUYlxXEVHinwjdAkb7ISOndL2oVWPg'
df_hearings = pd.read_csv(hearings_url)

print(f"üìÅ Cases Dataset: {df_cases.shape[0]:,} rows √ó {df_cases.shape[1]} columns")
print(f"üìÅ Hearings Dataset: {df_hearings.shape[0]:,} rows √ó {df_hearings.shape[1]} columns")

# Display first few rows
print("\nüîç Cases Data Preview:")
display(df_cases.head())

print("\nüîç Hearings Data Preview:")
display(df_hearings.head())

"""---

## üßπ 3. Data Cleaning & Preparation
"""

# Convert date columns to datetime
date_columns_cases = ['DATE_FILED', 'DECISION_DATE', 'REGISTRATION_DATE']
for col in date_columns_cases:
    if col in df_cases.columns:
        df_cases[col] = pd.to_datetime(df_cases[col], errors='coerce')

date_columns_hearings = ['BusinessOnDate', 'AppearanceDate']
for col in date_columns_hearings:
    if col in df_hearings.columns:
        df_hearings[col] = pd.to_datetime(df_hearings[col], errors='coerce')

# Create derived features for cases
df_cases['FILING_YEAR'] = df_cases['DATE_FILED'].dt.year
df_cases['DECISION_YEAR'] = df_cases['DECISION_DATE'].dt.year
df_cases['FILING_MONTH'] = df_cases['DATE_FILED'].dt.to_period('M').astype(str)

# Calculate disposal time if not present
if 'DISPOSALTIME_ADJ' not in df_cases.columns:
    df_cases['DISPOSALTIME_ADJ'] = (df_cases['DECISION_DATE'] - df_cases['DATE_FILED']).dt.days + 1

# Create age categories
def categorize_age(days):
    if pd.isna(days):
        return 'Unknown'
    elif days <= 365:
        return '0-1 year'
    elif days <= 730:
        return '1-2 years'
    elif days <= 1095:
        return '2-3 years'
    else:
        return '3+ years'

df_cases['AGE_CATEGORY'] = df_cases['DISPOSALTIME_ADJ'].apply(categorize_age)

# Clean outcomes
df_cases['NATURE_OF_DISPOSAL_OUTCOME'] = df_cases['NATURE_OF_DISPOSAL_OUTCOME'].fillna('Not Specified')

# Add risk flag for long pending cases
df_cases['HIGH_DELAY_RISK'] = df_cases['DISPOSALTIME_ADJ'] > 730  # > 2 years

print("‚úÖ Data cleaning complete!")
print(f"\nüìä Date range: {df_cases['DATE_FILED'].min()} to {df_cases['DECISION_DATE'].max()}")
print(f"‚è±Ô∏è Average disposal time: {df_cases['DISPOSALTIME_ADJ'].mean():.0f} days ({df_cases['DISPOSALTIME_ADJ'].mean()/365:.1f} years)")

"""---

## üìà 4. Executive Summary Dashboard
"""

# Calculate KPIs
total_cases = len(df_cases)
avg_disposal_days = df_cases['DISPOSALTIME_ADJ'].mean()
median_disposal_days = df_cases['DISPOSALTIME_ADJ'].median()
cases_over_1yr = (df_cases['DISPOSALTIME_ADJ'] > 365).sum()
cases_over_2yr = (df_cases['DISPOSALTIME_ADJ'] > 730).sum()
pct_over_1yr = (cases_over_1yr / total_cases) * 100

# Create KPI cards
fig = make_subplots(
    rows=2, cols=3,
    subplot_titles=('Total Disposed Cases', 'Avg Disposal Time', 'Median Disposal Time',
                    'Cases > 1 Year', 'Cases > 2 Years', '% Cases > 1 Year'),
    specs=[[{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}],
           [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}]]
)

# KPI 1: Total Cases
fig.add_trace(go.Indicator(
    mode="number",
    value=total_cases,
    number={'font': {'size': 60}},
), row=1, col=1)

# KPI 2: Avg Disposal Time
fig.add_trace(go.Indicator(
    mode="number+delta",
    value=avg_disposal_days,
    delta={'reference': 365, 'relative': False, 'valueformat': '.0f'},
    number={'suffix': ' days', 'font': {'size': 50}},
), row=1, col=2)

# KPI 3: Median Disposal Time
fig.add_trace(go.Indicator(
    mode="number",
    value=median_disposal_days,
    number={'suffix': ' days', 'font': {'size': 50}},
), row=1, col=3)

# KPI 4: Cases > 1 Year
fig.add_trace(go.Indicator(
    mode="number",
    value=cases_over_1yr,
    number={'font': {'size': 50, 'color': 'orange'}},
), row=2, col=1)

# KPI 5: Cases > 2 Years
fig.add_trace(go.Indicator(
    mode="number",
    value=cases_over_2yr,
    number={'font': {'size': 50, 'color': 'red'}},
), row=2, col=2)

# KPI 6: % Cases > 1 Year
fig.add_trace(go.Indicator(
    mode="gauge+number",
    value=pct_over_1yr,
    number={'suffix': '%'},
    gauge={'axis': {'range': [None, 100]},
           'bar': {'color': "darkred"},
           'steps': [
               {'range': [0, 30], 'color': "lightgreen"},
               {'range': [30, 60], 'color': "yellow"},
               {'range': [60, 100], 'color': "lightcoral"}],
           'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 70}}
), row=2, col=3)

fig.update_layout(
    height=600,
    title_text="üìä Executive Summary - Karnataka High Court (2015-2019)",
    title_font_size=24,
    showlegend=False
)

fig.show()

print(f"\nüìå Key Insights:")
print(f"   ‚Ä¢ {pct_over_1yr:.1f}% of cases took more than 1 year to dispose")
print(f"   ‚Ä¢ {(cases_over_2yr/total_cases)*100:.1f}% of cases took more than 2 years")
print(f"   ‚Ä¢ Fastest case: {df_cases['DISPOSALTIME_ADJ'].min()} days")
print(f"   ‚Ä¢ Longest case: {df_cases['DISPOSALTIME_ADJ'].max()} days ({df_cases['DISPOSALTIME_ADJ'].max()/365:.1f} years)")

"""---

## üîÑ 5. Case Progression Funnel
"""

# Count hearings by stage
stage_counts = df_hearings['Remappedstages'].value_counts()

# Define stage order for funnel
stage_order = ['ADMISSION', 'ORDERS / JUDGMENT', 'DISPOSED', 'NA']
stage_data = []

for stage in stage_order:
    if stage in stage_counts.index:
        stage_data.append({'Stage': stage, 'Count': stage_counts[stage]})

stage_df = pd.DataFrame(stage_data)

# Create funnel chart
fig = go.Figure(go.Funnel(
    y=stage_df['Stage'],
    x=stage_df['Count'],
    textposition="inside",
    textinfo="value+percent initial",
    marker={"color": ["#3498db", "#2ecc71", "#27ae60", "#95a5a6"]},
    connector={"line": {"color": "royalblue", "dash": "dot", "width": 3}}
))

fig.update_layout(
    title="üîÑ Case Progression Funnel - Hearing Stages Distribution",
    title_font_size=20,
    height=500
)

fig.show()

print(f"\nüîç Stage Analysis:")
print(f"   ‚Ä¢ {stage_counts.get('ADMISSION', 0):,} hearings at ADMISSION stage ({stage_counts.get('ADMISSION', 0)/len(df_hearings)*100:.1f}%)")
print(f"   ‚Ä¢ {stage_counts.get('ORDERS / JUDGMENT', 0):,} hearings at ORDERS/JUDGMENT stage")
print(f"   ‚Ä¢ {stage_counts.get('DISPOSED', 0):,} final disposals recorded in hearings")

"""---

## üö® 6. Bottleneck Analysis - Stage Duration Heatmap
"""

# Merge cases with hearings to analyze stage durations
df_merged = df_hearings.merge(
    df_cases[['COMBINED_CASE_NUMBER', 'DISPOSALTIME_ADJ', 'FILING_YEAR']],
    left_on='CombinedCaseNumber',
    right_on='COMBINED_CASE_NUMBER',
    how='left'
)

# Group by stage and calculate avg disposal time
stage_duration = df_merged.groupby('Remappedstages')['DISPOSALTIME_ADJ'].agg(['mean', 'median', 'count']).reset_index()
stage_duration = stage_duration[stage_duration['Remappedstages'] != 'NA'].sort_values('mean', ascending=False)

# Create bar chart for bottlenecks
fig = go.Figure()

fig.add_trace(go.Bar(
    x=stage_duration['Remappedstages'],
    y=stage_duration['mean'],
    name='Average Days',
    marker_color='indianred',
    text=stage_duration['mean'].round(0),
    textposition='outside'
))

fig.add_trace(go.Bar(
    x=stage_duration['Remappedstages'],
    y=stage_duration['median'],
    name='Median Days',
    marker_color='lightsalmon',
    text=stage_duration['median'].round(0),
    textposition='outside'
))

fig.update_layout(
    title="üö® Bottleneck Analysis - Average Days by Stage",
    xaxis_title="Case Stage",
    yaxis_title="Days to Disposal",
    barmode='group',
    height=500,
    title_font_size=20
)

fig.show()

print(f"\n‚ö†Ô∏è Bottleneck Insights:")
for idx, row in stage_duration.head(3).iterrows():
    print(f"   ‚Ä¢ {row['Remappedstages']}: Avg {row['mean']:.0f} days, {row['count']:.0f} hearings")

"""---

## ‚è∞ 7. Case Aging Alert System
"""

# Create aging categories
aging_alerts = df_cases.groupby('AGE_CATEGORY').size().reset_index(name='Count')
aging_alerts = aging_alerts.sort_values('Count', ascending=True)

# Horizontal bar chart for aging
fig = px.bar(
    aging_alerts,
    y='AGE_CATEGORY',
    x='Count',
    orientation='h',
    title='‚è∞ Case Aging Distribution',
    labels={'AGE_CATEGORY': 'Time to Disposal', 'Count': 'Number of Cases'},
    color='Count',
    color_continuous_scale='Reds',
    text='Count'
)

fig.update_traces(textposition='outside')
fig.update_layout(height=400, title_font_size=20, showlegend=False)
fig.show()

# Alert table for cases over 2 years
high_risk_cases = df_cases[df_cases['HIGH_DELAY_RISK'] == True][
    ['COMBINED_CASE_NUMBER', 'DATE_FILED', 'DECISION_DATE', 'DISPOSALTIME_ADJ',
     'COURT_NUMBER', 'NATURE_OF_DISPOSAL_OUTCOME']
].sort_values('DISPOSALTIME_ADJ', ascending=False).head(20)

high_risk_cases['DISPOSALTIME_YEARS'] = (high_risk_cases['DISPOSALTIME_ADJ'] / 365).round(1)

print(f"\nüö® HIGH PRIORITY ALERTS: {(df_cases['HIGH_DELAY_RISK'] == True).sum()} cases took > 2 years")
print("\nTop 20 Longest Cases:")
display(high_risk_cases[['COMBINED_CASE_NUMBER', 'DATE_FILED', 'DECISION_DATE',
                         'DISPOSALTIME_YEARS', 'COURT_NUMBER', 'NATURE_OF_DISPOSAL_OUTCOME']])

"""---

## üìÖ 8. Workload Timeline Analysis
"""

# Monthly filing vs disposal trends
monthly_filed = df_cases.groupby(df_cases['DATE_FILED'].dt.to_period('M')).size().reset_index(name='Filed')
monthly_filed.rename(columns={'DATE_FILED': 'Month_Period'}, inplace=True)

monthly_disposed = df_cases.groupby(df_cases['DECISION_DATE'].dt.to_period('M')).size().reset_index(name='Disposed')
monthly_disposed.rename(columns={'DECISION_DATE': 'Month_Period'}, inplace=True)

# Merge on the Period objects
monthly_trends = monthly_filed.merge(monthly_disposed, on='Month_Period', how='outer')
monthly_trends[['Filed', 'Disposed']] = monthly_trends[['Filed', 'Disposed']].fillna(0)

# Sort by the Period objects for correct chronological order
monthly_trends = monthly_trends.sort_values('Month_Period')

# Convert to string for plotting if plotly requires it, or use Period directly if supported
monthly_trends['Month_Str'] = monthly_trends['Month_Period'].astype(str)

# Create line chart
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=monthly_trends['Month_Str'], # Use the string representation for plotting
    y=monthly_trends['Filed'],
    mode='lines+markers',
    name='Cases Filed',
    line=dict(color='royalblue', width=3)
))

fig.add_trace(go.Scatter(
    x=monthly_trends['Month_Str'], # Use the string representation for plotting
    y=monthly_trends['Disposed'],
    mode='lines+markers',
    name='Cases Disposed',
    line=dict(color='green', width=3)
))

fig.update_layout(
    title="üìÖ Monthly Filing vs Disposal Trends (2015-2020)",
    xaxis_title="Month",
    yaxis_title="Number of Cases",
    height=500,
    title_font_size=20,
    hovermode='x unified'
)

fig.show()

# Identify backlog periods
monthly_trends['Backlog'] = monthly_trends['Filed'] - monthly_trends['Disposed']
backlog_periods = monthly_trends[monthly_trends['Backlog'] > 0].sort_values('Backlog', ascending=False).head(5)

print(f"\nüìä Backlog Insights:")
print(f"   ‚Ä¢ Peak backlog month: {backlog_periods.iloc[0]['Month_Str']} (+{backlog_periods.iloc[0]['Backlog']:.0f} cases)")
print(f"   ‚Ä¢ Total months with filing > disposal: {(monthly_trends['Backlog'] > 0).sum()}")

"""---

## üéØ 9. Court & Act-wise Distribution
"""

import plotly.io as pio
import plotly.express as px
import matplotlib.pyplot as plt

pio.renderers.default = 'colab'

# Court-wise distribution
court_dist = df_cases['COURT_NUMBER'].value_counts().head(10).reset_index()
court_dist.columns = ['Court', 'Cases']

# Use Matplotlib for the bar chart
print("### Top 10 Courts by Case Volume (Matplotlib):")
if not court_dist.empty:
    plt.figure(figsize=(12, 7))
    plt.bar(court_dist['Court'].astype(str), court_dist['Cases'], color='skyblue')
    plt.xlabel('Court Number')
    plt.ylabel('Number of Cases')
    plt.title('üìç Top 10 Courts by Case Volume (Matplotlib)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
else:
    print("No court distribution data available to display with Matplotlib.")

# Acts distribution (clean the data first) - still using Plotly
acts_clean = df_cases[df_cases['UNDER_ACTS'].notna() & (df_cases['UNDER_ACTS'] != 'NA')]
if len(acts_clean) > 0:
    acts_dist = acts_clean['UNDER_ACTS'].value_counts().head(10).reset_index()
    acts_dist.columns = ['Act', 'Cases']

    fig = px.pie(
        acts_dist,
        names='Act',
        values='Cases',
        title='‚öñÔ∏è Top Legal Acts Distribution',
        hole=0.4
    )
    fig.update_layout(height=500, title_font_size=20)
    fig.show()
else:
    print("No legal acts data available to display.")

"""---

## ü§ñ 10. ML Prediction Model

### Predicting High Delay Risk (Cases > 2 Years)

####10(a). Logistic Learninig (Initial)
"""

print("ü§ñ Building Logistic Machine Learning Model...")

# Prepare features
ml_data = df_cases[['FILING_YEAR', 'COURT_NUMBER', 'UNDER_ACTS', 'HIGH_DELAY_RISK']].copy()
ml_data = ml_data.dropna(subset=['FILING_YEAR', 'COURT_NUMBER'])

# Encode categorical variables
le_court = LabelEncoder()
ml_data['COURT_ENCODED'] = le_court.fit_transform(ml_data['COURT_NUMBER'].astype(str))

# For acts - handle NAs
ml_data['UNDER_ACTS'] = ml_data['UNDER_ACTS'].fillna('Not Specified')
le_acts = LabelEncoder()
ml_data['ACTS_ENCODED'] = le_acts.fit_transform(ml_data['UNDER_ACTS'])

# Features and target
X = ml_data[['FILING_YEAR', 'COURT_ENCODED', 'ACTS_ENCODED']]
y = ml_data['HIGH_DELAY_RISK'].astype(int)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Train model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n‚úÖ Model trained successfully!")
print(f"üìä Model Accuracy: {accuracy*100:.2f}%")
print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal (<2 yrs)', 'High Risk (>2 yrs)']))

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': ['Filing Year', 'Court', 'Legal Act'],
    'Coefficient': model.coef_[0]
}).sort_values('Coefficient', ascending=False)

fig = px.bar(
    feature_importance,
    x='Coefficient',
    y='Feature',
    orientation='h',
    title='üéØ Feature Importance for Delay Prediction',
    color='Coefficient',
    color_continuous_scale='RdYlGn_r',
    text='Coefficient'
)

fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')
fig.update_layout(height=400, title_font_size=20)
fig.show()

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
fig = px.imshow(
    cm,
    labels=dict(x="Predicted", y="Actual", color="Count"),
    x=['Normal', 'High Risk'],
    y=['Normal', 'High Risk'],
    title='üéØ Confusion Matrix',
    text_auto=True,
    color_continuous_scale='Blues'
)
fig.update_layout(height=400, title_font_size=20)
fig.show()

print(f"\nüí° Model Insights:")
print(f"   ‚Ä¢ The model can predict delay risk with {accuracy*100:.1f}% accuracy")
print(f"   ‚Ä¢ Most important factor: {feature_importance.iloc[0]['Feature']}")
print(f"   ‚Ä¢ This can help prioritize cases likely to face delays")

"""####10(b). Logistic Learninig (Final)"""

print("ü§ñ Implementing Final Improved Logistic Regression Model...")

# --- Start of added code to create interaction features ---
from sklearn.preprocessing import PolynomialFeatures

# Create PolynomialFeatures to generate interaction terms
# degree=2 will create all polynomial combinations of the features with degree less than or equal to 2.
# include_bias=False prevents adding a column of all ones, as LogisticRegression typically handles its own intercept.
poly = PolynomialFeatures(degree=2, include_bias=False)

# Fit and transform on training data
X_train_interact = pd.DataFrame(poly.fit_transform(X_train),
                                 columns=poly.get_feature_names_out(X_train.columns),
                                 index=X_train.index)

# Transform test data (use the fitted poly object from training data to avoid data leakage)
X_test_interact = pd.DataFrame(poly.transform(X_test),
                               columns=poly.get_feature_names_out(X_test.columns),
                               index=X_test.index)

print(f"‚úÖ Interaction features created. X_train_interact shape: {X_train_interact.shape}")
print(f"‚úÖ Interaction features created. X_test_interact shape: {X_test_interact.shape}")
# --- End of added code ---

# Initialize Logistic Regression model with class_weight='balanced'
model_balanced_final_lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')

# Train model with interaction features
model_balanced_final_lr.fit(X_train_interact, y_train)

# Predictions
y_pred_balanced_final_lr = model_balanced_final_lr.predict(X_test_interact)
accuracy_balanced_final_lr = accuracy_score(y_test, y_pred_balanced_final_lr)

print(f"\n‚úÖ Final Logistic Regression Model trained successfully!")
print(f"üìä Model Accuracy (LR with interactions & balanced weight): {accuracy_balanced_final_lr*100:.2f}%")
print("\nüìã Classification Report (LR with interactions & balanced weight):")
print(classification_report(y_test, y_pred_balanced_final_lr, target_names=['Normal (<2 yrs)', 'High Risk (>2 yrs)']))

# Confusion matrix
cm_balanced_final_lr = confusion_matrix(y_test, y_pred_balanced_final_lr)
fig_cm_balanced_final_lr = px.imshow(
    cm_balanced_final_lr,
    labels=dict(x="Predicted", y="Actual", color="Count"),
    x=['Normal', 'High Risk'],
    y=['Normal', 'High Risk'],
    title='üéØ Confusion Matrix (Final LR with interactions & balanced weight)',
    text_auto=True,
    color_continuous_scale='Blues'
)
fig_cm_balanced_final_lr.update_layout(height=400, title_font_size=20)
fig_cm_balanced_final_lr.show()

# Feature importance
feature_importance_lr = pd.DataFrame({
    'Feature': X_train_interact.columns,
    'Coefficient': model_balanced_final_lr.coef_[0]
}).sort_values('Coefficient', ascending=False)

fig_feat_imp_lr = px.bar(
    feature_importance_lr,
    x='Coefficient',
    y='Feature',
    orientation='h',
    title='üéØ Feature Importance (Final LR with interactions & balanced weight)',
    color='Coefficient',
    color_continuous_scale='RdBu',
    text='Coefficient'
)

fig_feat_imp_lr.update_traces(texttemplate='%{text:.3f}', textposition='outside')
fig_feat_imp_lr.update_layout(height=400, title_font_size=20)
fig_feat_imp_lr.show()

"""####10(c). Decision Tree (Initial)"""

print("ü§ñ Building Decision Tree Machine Learning Model...")

from sklearn.tree import DecisionTreeClassifier

# Reuse data split from previous model
# X_train, X_test, y_train, y_test are already available

# Train Decision Tree model
dtc_model = DecisionTreeClassifier(random_state=42)
dtc_model.fit(X_train, y_train)

# Predictions
y_pred_dtc = dtc_model.predict(X_test)
accuracy_dtc = accuracy_score(y_test, y_pred_dtc)

print(f"\n‚úÖ Decision Tree model trained successfully!")
print(f"üìä Decision Tree Model Accuracy: {accuracy_dtc*100:.2f}%")
print("\nüìã Decision Tree Classification Report:")
print(classification_report(y_test, y_pred_dtc, target_names=['Normal (<2 yrs)', 'High Risk (>2 yrs)']))

# Feature importance for Decision Tree
feature_importance_dtc = pd.DataFrame({
    'Feature': ['Filing Year', 'Court', 'Legal Act'],
    'Importance': dtc_model.feature_importances_
}).sort_values('Importance', ascending=False)

fig_dtc_feat_imp = px.bar(
    feature_importance_dtc,
    x='Importance',
    y='Feature',
    orientation='h',
    title='üéØ Decision Tree Feature Importance for Delay Prediction',
    color='Importance',
    color_continuous_scale='Plasma',
    text='Importance'
)

fig_dtc_feat_imp.update_traces(texttemplate='%{text:.3f}', textposition='outside')
fig_dtc_feat_imp.update_layout(height=400, title_font_size=20)
fig_dtc_feat_imp.show()

# Confusion matrix for Decision Tree
cm_dtc = confusion_matrix(y_test, y_pred_dtc)
fig_dtc_cm = px.imshow(
    cm_dtc,
    labels=dict(x="Predicted", y="Actual", color="Count"),
    x=['Normal', 'High Risk'],
    y=['Normal', 'High Risk'],
    title='üéØ Decision Tree Confusion Matrix',
    text_auto=True,
    color_continuous_scale='Viridis'
)
fig_dtc_cm.update_layout(height=400, title_font_size=20)
fig_dtc_cm.show()

print(f"\nüí° Decision Tree Model Insights:")
print(f"   ‚Ä¢ The Decision Tree model predicts delay risk with {accuracy_dtc*100:.1f}% accuracy")
print(f"   ‚Ä¢ Most important factor: {feature_importance_dtc.iloc[0]['Feature']}")
print(f"   ‚Ä¢ Decision Trees can capture non-linear relationships in data")

"""####10(d). Decision Tree (Final)"""

print("ü§ñ Implementing Final Improved Decision Tree Model...")

# Initialize Decision Tree model with class_weight='balanced'
dt_model_balanced_final = DecisionTreeClassifier(random_state=42, class_weight='balanced')

# Define parameter grid for GridSearchCV (Best parameters from previous run: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5})
param_grid = {
    'max_depth': [3, 5, 7, 10, None], # None means unlimited depth
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Initialize GridSearchCV to optimize for f1-score of the 'High Risk' class (positive class)
grid_search_final = GridSearchCV(
    estimator=dt_model_balanced_final,
    param_grid=param_grid,
    scoring='f1_macro', # Use macro f1 to consider both classes equally
    cv=3, # 3-fold cross-validation
    n_jobs=-1, # Use all available cores
    verbose=0 # Suppress verbose output during grid search for cleaner output
)

# Fit GridSearchCV to the training data with interaction features
grid_search_final.fit(X_train_interact, y_train)

# Get the best estimator
best_dt_model_final = grid_search_final.best_estimator_

# Predictions with the best model
y_pred_best_dt_final = best_dt_model_final.predict(X_test_interact)
accuracy_best_dt_final = accuracy_score(y_test, y_pred_best_dt_final)

print(f"\n‚úÖ Best Decision Tree Model trained successfully!\nBest parameters: {grid_search_final.best_params_}")
print(f"üìä Best Decision Tree Model Accuracy (with interactions & balanced weight): {accuracy_best_dt_final*100:.2f}%")
print("\nüìã Classification Report (Best Decision Tree with interactions & balanced weight):")
print(classification_report(y_test, y_pred_best_dt_final, target_names=['Normal (<2 yrs)', 'High Risk (>2 yrs)']))

# Confusion matrix for the best Decision Tree
cm_best_dt_final = confusion_matrix(y_test, y_pred_best_dt_final)
fig_cm_best_dt_final = px.imshow(
    cm_best_dt_final,
    labels=dict(x="Predicted", y="Actual", color="Count"),
    x=['Normal', 'High Risk'],
    y=['Normal', 'High Risk'],
    title='üéØ Confusion Matrix (Final Best Decision Tree with interactions & balanced weight)',
    text_auto=True,
    color_continuous_scale='Viridis'
)
fig_cm_best_dt_final.update_layout(height=400, title_font_size=20)
fig_cm_best_dt_final.show()

# Feature importance for the best Decision Tree
feature_importance_dt_final = pd.DataFrame({
    'Feature': X_train_interact.columns,
    'Importance': best_dt_model_final.feature_importances_
}).sort_values('Importance', ascending=False)

fig_dt_feat_imp_final = px.bar(
    feature_importance_dt_final,
    x='Importance',
    y='Feature',
    orientation='h',
    title='üéØ Feature Importance (Final Best Decision Tree with interactions & balanced weight)',
    color='Importance',
    color_continuous_scale='Plasma',
    text='Importance'
)
fig_dt_feat_imp_final.update_traces(texttemplate='%{text:.3f}', textposition='outside')
fig_dt_feat_imp_final.update_layout(height=400, title_font_size=20)
fig_dt_feat_imp_final.show()

"""---

## üìä 11. Summary & Recommendations

### Key Findings:

1. **Bottleneck Identification**: ADMISSION stage dominates (~80% of hearings), indicating pre-trial delays
2. **Case Aging**: 71.1% of cases took more than 1 year to dispose
3. **Workload Trends**: Significant backlog accumulation in 2015-2016
4. **Predictive Insights**: ML model shows filing year and court are key delay predictors

### Recommendations for Judges:

‚úÖ **Prioritize Admission Stage**: Implement fast-track admission hearings for cases pending >6 months  
‚úÖ **Court Workload Balancing**: Redistribute cases from high-volume courts (see Court Distribution chart)  
‚úÖ **Early Warning System**: Flag cases at risk of delay using ML predictions  
‚úÖ **Monthly Review**: Track filing vs disposal trends to prevent backlog accumulation

### Recommendations for Lawyers:

‚úÖ **Case Portfolio Monitoring**: Track your cases through the funnel stages  
‚úÖ **Proactive Preparation**: Cases in ADMISSION stage for >3 months need urgent follow-up  
‚úÖ **Timeline Awareness**: Average disposal time is '''672 days - plan accordingly

### Dashboard Impact:

üìà **For NJDG Platform**: This transforms NJDG from a reporting tool to an active case management system  
‚ö° **For Justice Delivery**: Enables data-driven decision making and bottleneck resolution  
üéØ **For Stakeholders**: Provides actionable insights for judges, lawyers, and court administrators

---

**Dashboard Created by:** Lawgorithm  
**Date:** Nov 30, 2025
**Data Source:** Karnataka High Court (2015-2019, MFA Cases)
"""

print("\n" + "="*80)
print("‚úÖ DASHBOARD GENERATION COMPLETE!")
print("="*80)

"""### Additional Notes

## COMPARING MACHINE LEARNING MODELS
### Detailed Discussion
Comparing the performance of the original models with the improved versions (including class weighting and interaction features). Discuss accuracy, precision, recall, and f1-score for the 'High Risk' class, and explain the impact of the changes.

## Comparison: Logistic Regression Models

### Comparison of Logistic Regression Models

Let's compare the performance metrics for the 'High Risk (>2 yrs)' class and the overall accuracy of both Logistic Regression models:

**Original Logistic Regression Model (without balanced class weight and interaction features):**
- **Accuracy:** 63.94%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.00
    - **Recall:** 0.00
    - **F1-score:** 0.00

**Improved Logistic Regression Model (with balanced class weight and interaction features):**
- **Accuracy:** 57.02%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.44
    - **Recall:** 0.75
    - **F1-score:** 0.56

### Analysis and Impact of Changes:

1.  **Accuracy:** The overall accuracy of the model decreased from 63.94% in the original model to 57.02% in the improved model. This might initially seem like a downgrade, but it's crucial to look at the class-specific metrics, especially for imbalanced datasets.

2.  **Precision for 'High Risk' Class:** The precision for the 'High Risk' class dramatically improved from 0.00 in the original model to 0.44 in the improved model. This means that when the improved model predicts a case as 'High Risk', it is correct 44% of the time. The original model made almost no correct positive predictions.

3.  **Recall for 'High Risk' Class:** The recall for the 'High Risk' class also saw a significant improvement, from 0.00 to 0.75. This indicates that the improved model is able to identify 75% of all actual 'High Risk' cases. The original model failed to identify any 'High Risk' cases.

4.  **F1-score for 'High Risk' Class:** The F1-score, which is the harmonic mean of precision and recall, improved from 0.00 to 0.56. This is a much more robust metric for evaluating performance on imbalanced classes and shows a substantial improvement in the model's ability to balance precision and recall for the 'High Risk' class.

**Impact of Changes (Class Weighting and Interaction Features):**

*   **Addressing Class Imbalance:** The most significant impact comes from incorporating `class_weight='balanced'`. The original model, without this, primarily predicted the majority class ('Normal (<2 yrs)') for almost all instances, leading to high accuracy but completely failing to identify the minority 'High Risk' class (recall of 0.00). By balancing the class weights, the model is penalized more for misclassifying the minority class, forcing it to learn patterns for 'High Risk' cases.
*   **Improved 'High Risk' Detection:** As a result, the improved model, despite a lower overall accuracy, is far superior in identifying 'High Risk' cases, which is often the primary objective in such prediction tasks (e.g., flagging cases that need intervention). The recall of 0.75 is a strong indicator of this improved detection capability.
*   **Interaction Features:** While the `class_weight` parameter likely had the most profound impact, the addition of interaction features (`FILING_COURT_INTERACT`, `FILING_ACTS_INTERACT`, `COURT_ACTS_INTERACT`, `ALL_THREE_INTERACT`) allowed the model to capture more complex relationships between the features. This can help the model differentiate between cases that might seem similar based on individual features but behave differently due to their combined effect, further contributing to better class separation and prediction for the minority class. This is evident from the feature importance plot for the improved model, where interaction terms appear to be significant.

In conclusion, while the overall accuracy dropped, the improved model is significantly more effective and useful for its intended purpose: identifying 'High Risk' cases, thanks to the balanced class weighting and the inclusion of interaction features.

## Comparison: Decision Tree Models

### Comparison of Decision Tree Models

Let's compare the performance metrics for the 'High Risk (>2 yrs)' class and the overall accuracy of both Decision Tree models:

**Original Decision Tree Model (without balanced class weight, interaction features, or hyperparameter tuning):**
- **Accuracy:** 84.15%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.93
    - **Recall:** 0.61
    - **F1-score:** 0.73

**Improved Decision Tree Model (with balanced class weight, interaction features, and hyperparameter tuning):**
- **Accuracy:** 84.07%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.90
    - **Recall:** 0.63
    - **F1-score:** 0.74

### Analysis and Impact of Enhancements:

1.  **Overall Accuracy:** The overall accuracy remained largely stable, decreasing slightly from 84.15% to 84.07%. For Decision Trees, which can capture complex relationships, the baseline performance was already quite high. This stability suggests that the enhancements did not significantly boost overall accuracy but aimed to refine performance for specific classes.

2.  **Precision for 'High Risk' Class:** Precision for the 'High Risk' class slightly decreased from 0.93 to 0.90. This means that when the improved model predicts a case as 'High Risk', it is slightly less precise, with 90% of those predictions being correct, compared to 93% in the original model.

3.  **Recall for 'High Risk' Class:** Recall for the 'High Risk' class slightly improved from 0.61 to 0.63. This indicates that the improved model is slightly better at identifying actual 'High Risk' cases, capturing 63% of them compared to 61% by the original model. This is a positive change, as often in imbalanced classification, increasing recall for the minority class is a key objective.

4.  **F1-score for 'High Risk' Class:** The F1-score for the 'High Risk' class saw a minor improvement from 0.73 to 0.74. This indicates a slightly better balance between precision and recall for the minority class in the improved model.

**Impact of Enhancements (Class Weighting, Interaction Features, and Hyperparameter Tuning):**

*   **Class Weighting (`class_weight='balanced'`):** For Decision Trees, balancing class weights typically helps the model pay more attention to the minority class. In this case, it likely contributed to the slight increase in recall for the 'High Risk' class, ensuring the model doesn't solely optimize for the majority class.

*   **Interaction Features:** The addition of interaction features (`FILING_COURT_INTERACT`, `FILING_ACTS_INTERACT`, `COURT_ACTS_INTERACT`, `ALL_THREE_INTERACT`) allowed the model to consider more nuanced relationships between variables. The feature importance plot for the improved model showed that some interaction terms became quite significant. This can lead to a more robust decision-making process by capturing combined effects that individual features might miss. For instance, a particular `FILING_YEAR` combined with a specific `COURT_NUMBER` might create a higher risk of delay than either factor alone.

*   **Hyperparameter Tuning (GridSearchCV):** Hyperparameter tuning (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion`) helps in finding the optimal configuration for the Decision Tree. In the original model, default parameters might not have been ideal. Tuning ensures that the tree is neither too shallow (underfitting) nor too deep (overfitting), leading to a better generalization capability. The final best parameters indicate a refined structure for the tree, which is crucial for optimal performance.

In summary, while the Decision Tree model already performed well initially, the enhancements led to a slight improvement in the detection of 'High Risk' cases (recall and F1-score) and provided a more robust and finely tuned model by considering class imbalance, complex feature interactions, and optimized hyperparameters. This is particularly valuable when identifying the minority 'High Risk' cases is critical, even if it means a very slight trade-off in overall accuracy or precision for that class.

# Compare Best Logistic Regression vs. Best Decision Tree
Comparing the final improved Logistic Regression and Decision Tree models for 'High Delay Risk' prediction, refining discussions on feature selection/dropping, consolidating all necessary code and analysis into a clean notebook, and confirming its completeness.

### Comparison of Final Logistic Regression vs. Final Decision Tree Models

Let's compare the performance metrics for the 'High Risk (>2 yrs)' class and the overall accuracy of the final, improved Logistic Regression and Decision Tree models.

**Final Improved Logistic Regression Model:**
- **Overall Accuracy:** 57.02%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.44
    - **Recall:** 0.75
    - **F1-score:** 0.56

**Final Improved Decision Tree Model:**
- **Overall Accuracy:** 84.07%
- **'High Risk (>2 yrs)' Class:**
    - **Precision:** 0.90
    - **Recall:** 0.63
    - **F1-score:** 0.74

### Analysis and Conclusion:

1.  **Overall Accuracy:** The Decision Tree model significantly outperforms the Logistic Regression model in overall accuracy (84.07% vs. 57.02%). This suggests that the Decision Tree generalizes better across both classes combined.

2.  **Precision for 'High Risk' Class:** The Decision Tree model has a much higher precision (0.90) for the 'High Risk' class compared to Logistic Regression (0.44). This means when the Decision Tree predicts a case is 'High Risk', it is correct 90% of the time, while the Logistic Regression model is correct only 44% of the time. High precision is crucial if false positives (flagging a normal case as high risk) are costly or lead to unnecessary interventions.

3.  **Recall for 'High Risk' Class:** The Logistic Regression model achieves a higher recall (0.75) for the 'High Risk' class compared to the Decision Tree (0.63). This means the Logistic Regression model is better at identifying a larger proportion of the actual 'High Risk' cases. High recall is important when missing an actual 'High Risk' case (false negative) has severe consequences.

4.  **F1-score for 'High Risk' Class:** The Decision Tree model has a slightly higher F1-score (0.74 vs. 0.56), indicating a better balance between precision and recall for the 'High Risk' class overall.

### Which Model is Superior for Identifying 'High Delay Risk' Cases?

The **Decision Tree Model is superior** for identifying 'High Delay Risk' cases in this context, primarily due to its significantly higher overall accuracy and much higher precision for the 'High Risk' class, combined with a competitive F1-score.

While the Logistic Regression had higher recall, indicating it caught more of the high-risk cases, its very low precision (44%) means a large number of the cases it flagged as high risk were actually not. This could lead to a lot of wasted resources investigating false alarms. In a scenario like case management where resources for intervention are limited, having high confidence that a flagged case is indeed high-risk (high precision) might be more valuable than catching every single high-risk case at the cost of many false positives. The Decision Tree offers a better balance: it identifies a good portion of high-risk cases (63% recall) and does so with high reliability (90% precision), minimizing unnecessary work. Furthermore, its higher overall accuracy suggests a more robust model across the entire dataset.

## Summary:

### Q&A
1.  **Which model is superior for identifying 'High Delay Risk' cases?**
    The Decision Tree model is superior for identifying 'High Delay Risk' cases in this context. While the Logistic Regression model achieved a higher recall for the 'High Risk' class (0.75), its precision was significantly lower (0.44), indicating many false positives. The Decision Tree model provided a better balance with a much higher precision (0.91) and an overall accuracy of 84.01%, making it more reliable for flagging cases that truly have a high delay risk and minimizing wasted resources on false alarms.

### Data Analysis Key Findings
*   The dataset comprises 20,653 case records and 101,058 hearing records.
*   The average disposal time for cases is 672 days (approximately 1.8 years), with the longest case taking over 6 years.
*   A significant proportion of cases experience prolonged delays: 71.1% of cases took more than 1 year, and 36.0% took more than 2 years to dispose.
*   The 'ADMISSION' stage accounts for a large majority (81.7%) of hearings, suggesting a potential bottleneck in early case progression.
*   Stages such as 'ARGUMENTS' (average 1424 days), 'OTHER' (average 1194 days), and 'ORDERS / JUDGMENT' (average 1023 days) are identified as major contributors to extended disposal times.
*   An alert system highlighted 7,429 cases that took longer than 2 years to dispose, indicating a substantial backlog of high-risk cases.
*   Monthly trends show that cases filed often exceeded cases disposed, with a peak backlog of +706 cases in April 2015 and 36 months where filings outpaced disposals.
*   For predicting 'High Delay Risk', the final improved **Logistic Regression model** achieved an overall accuracy of 57.02%. For the 'High Risk (>2 yrs)' class, it had a precision of 0.44, recall of 0.75, and an F1-score of 0.56. This was a substantial improvement from an initial Logistic Regression model that failed to identify any 'High Risk' cases (precision/recall of 0.00).
*   The final improved **Decision Tree model**, after hyperparameter tuning, achieved a higher overall accuracy of 84.01%. For the 'High Risk (>2 yrs)' class, it demonstrated a precision of 0.91, recall of 0.62, and an F1-score of 0.74.

### Insights or Next Steps
*   **Prioritize Decision Tree for Deployment:** Given its superior overall accuracy (84.01%) and high precision (0.91) for identifying 'High Delay Risk' cases, the Decision Tree model is better suited for practical application, especially where minimizing false positives is critical for efficient resource allocation in case management.
*   **Further Feature Exploration and Model Robustness:** Continue to explore domain-specific feature engineering and advanced feature selection techniques. Creating separate model versions with different feature subsets could offer insights into trade-offs between interpretability, robustness, and predictive power, especially when considering the use of models in judicial or administrative contexts.
"""

# @title
print("Thank You")

